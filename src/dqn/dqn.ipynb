{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import cv2\n",
    "from gymnasium import make, Env\n",
    "import timeit\n",
    "from random import random"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xa4rOyuRbt-l",
    "outputId": "1635c83c-b9b1-4d17-ccf9-d9c9c063c027"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "CONSTANTS"
   ],
   "metadata": {
    "id": "JMvIuY1ob_Y9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "CHANNELS = 4  # 4 if using stacked frames, 1 if not\n",
    "WINDOW_SIZE = 80  # We are using squared preprocessed images for the network, this is the size of the image in pixels.\n",
    "\n",
    "NUM_ACTIONS = 3  # Number of actions the agent can take\n",
    "INPUT_SHAPE = (CHANNELS, WINDOW_SIZE, WINDOW_SIZE)  # PyTorch uses (channels, height, width) format\n",
    "\n",
    "# TODO: Fine tuning\n",
    "LEARNING_RATE = 0.0005\n",
    "MIN_MEMORY_CAPACITY = 100  # This should be at least BATCH_SIZE\n",
    "MEMORY_CAPACITY = 100_000\n",
    "NUM_EPISODES = 200\n",
    "BATCH_SIZE = 32\n",
    "UPDATE_FREQUENCY = 1\n",
    "\n",
    "# These might be good\n",
    "GAMMA = 0.99\n",
    "EPSILON_MAX = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.995"
   ],
   "metadata": {
    "id": "_r7A5nczb2JU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "PREPROCESSING"
   ],
   "metadata": {
    "id": "L2z4gygEcRjh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def crop(frame: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Crops the frame image to the relevant part of the screen.\n",
    "    :param frame: the frame(state) image\n",
    "    :return: the cropped image\n",
    "    \"\"\"\n",
    "    # Exact crop [30:180, 8:152]\n",
    "    # Rounded crop [30:180, 10:150]\n",
    "    # Maybe try with both of them\n",
    "    return frame[30:180, 8:152]\n",
    "\n",
    "\n",
    "def resize(frame: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Resizes the frame image.\n",
    "    :param frame: the frame(state) image\n",
    "    :return: the resized image\n",
    "    \"\"\"\n",
    "    state = cv2.resize(frame, (WINDOW_SIZE, WINDOW_SIZE))\n",
    "    return state\n",
    "\n",
    "\n",
    "def rgb2gray(rgb: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts a rgb image array to a grey image array.\n",
    "\n",
    "    :param rgb: the rgb image array.\n",
    "    :return: the converted array.\n",
    "    \"\"\"\n",
    "    grayscale = cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY)\n",
    "    return grayscale\n",
    "\n",
    "\n",
    "def format2pytorch(frame: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Formats the frame image to be used with PyTorch. It does this by adding a new axis to the image array.\n",
    "    (int, int) -> (1, int, int) for PyTorch\n",
    "    :param frame: the frame(state) image\n",
    "    :return: the formatted image\n",
    "    \"\"\"\n",
    "    return frame[np.newaxis, :, :]\n",
    "\n",
    "\n",
    "def normalize(frame: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalizes the frame image.\n",
    "    :param frame: the frame(state) image\n",
    "    :return: the normalized image\n",
    "    \"\"\"\n",
    "    frame = frame.astype(np.float32)\n",
    "    frame /= 255.0\n",
    "    return frame\n",
    "\n",
    "\n",
    "def preprocess(frame: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocesses the frame image.\n",
    "    :param frame: the frame(state) image\n",
    "    :return: the preprocessed image\n",
    "    \"\"\"\n",
    "    frame = crop(frame)\n",
    "    frame = resize(frame)\n",
    "    frame = rgb2gray(frame)\n",
    "    # frame = format2pytorch(frame)\n",
    "    frame = normalize(frame)\n",
    "    return frame"
   ],
   "metadata": {
    "id": "GhZLP0SqcSni"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "REPLAY MEMORY"
   ],
   "metadata": {
    "id": "CNC6-CJYcArD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self):\n",
    "        self.capacity = MEMORY_CAPACITY\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.next_states = []\n",
    "        self.index: int = 0\n",
    "\n",
    "    def store(self, state, action, reward, done, next_state):\n",
    "        if len(self.states) < self.capacity:\n",
    "            self.states.append(state)\n",
    "            self.actions.append(action)\n",
    "            self.rewards.append(reward)\n",
    "            self.dones.append(done)\n",
    "            self.next_states.append(next_state)\n",
    "        else:\n",
    "            self.states[self.index] = state\n",
    "            self.actions[self.index] = action\n",
    "            self.rewards[self.index] = reward\n",
    "            self.dones[self.index] = done\n",
    "            self.next_states[self.index] = next_state\n",
    "\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "\n",
    "    def get_last3_frames(self):\n",
    "        return [self.states[-3], self.states[-2], self.states[-1]]\n",
    "\n",
    "    def _index_valid(self, index):\n",
    "        if any(self.dones[i] for i in range(index - 3, index + 1)):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def sample(self):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        next_states = []\n",
    "\n",
    "        while len(states) < BATCH_SIZE:\n",
    "            index = np.random.randint(4, len(self) - 1)\n",
    "            if self._index_valid(index):\n",
    "                states.append(\n",
    "                    [self.states[index - 3], self.states[index - 2], self.states[index - 1], self.states[index]]\n",
    "                )\n",
    "                next_states.append(\n",
    "                    [\n",
    "                        self.next_states[index - 2],\n",
    "                        self.next_states[index - 1],\n",
    "                        self.next_states[index],\n",
    "                        self.next_states[index + 1],\n",
    "                    ]\n",
    "                )\n",
    "                actions.append(self.actions[index])\n",
    "                rewards.append(self.rewards[index])\n",
    "                dones.append(self.dones[index])\n",
    "\n",
    "        states = torch.from_numpy(np.array(states)).float().to(DEVICE)\n",
    "        actions = torch.from_numpy(np.array(actions)).to(DEVICE).reshape((-1, 1))\n",
    "        rewards = torch.from_numpy(np.array(rewards)).float().to(DEVICE).reshape((-1, 1))\n",
    "        dones = torch.from_numpy(np.array(dones)).to(DEVICE).reshape((-1, 1))\n",
    "        next_states = torch.from_numpy(np.array(next_states)).float().to(DEVICE)\n",
    "\n",
    "        return states, actions, rewards, dones, next_states\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)"
   ],
   "metadata": {
    "id": "GubpTWwGb5IN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "NET"
   ],
   "metadata": {
    "id": "ekUT2KO7cJeg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(INPUT_SHAPE[0], 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc = nn.Linear(64 * 6 * 6, 512)\n",
    "        self.output = nn.Linear(512, NUM_ACTIONS)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # TODO: Maybe use RMSProp?\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE)\n",
    "        # self.optimizer = optim.RMSprop(self.parameters(), lr=LEARNING_RATE)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc(x))\n",
    "        x = self.output(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "id": "Mj6ltSIhcI2p"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "AGENT"
   ],
   "metadata": {
    "id": "EjsQNQgNcVkV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Agent:\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "        self.epsilon: float = EPSILON_MAX\n",
    "        self.replay_memory: ReplayMemory = ReplayMemory()\n",
    "\n",
    "        self.policy_net: DQN = DQN().to(DEVICE)\n",
    "        self.target_net: DQN = DQN().to(DEVICE)\n",
    "        self.update_target_net()\n",
    "\n",
    "        self.policy_net.train()\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.total_loss = 0.0\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random() < self.epsilon:\n",
    "            return self.action_space.sample()\n",
    "\n",
    "        last3_frames = self.replay_memory.get_last3_frames()\n",
    "        stacked_frames = last3_frames + [state]\n",
    "        stacked_frames = torch.from_numpy(np.array(stacked_frames)).float().unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action = torch.argmax(self.policy_net(stacked_frames))\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon * EPSILON_DECAY, EPSILON_MIN)\n",
    "\n",
    "    def set_loss(self, loss):\n",
    "        self.total_loss = loss\n",
    "\n",
    "    def please_learn(self):\n",
    "        if len(self.replay_memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, dones, next_states = self.replay_memory.sample()\n",
    "\n",
    "        predicted_qs = self.policy_net(states).gather(1, actions)\n",
    "        target_qs = self.target_net(next_states)\n",
    "        target_qs = torch.max(target_qs, dim=1).values.reshape(-1, 1)\n",
    "        target_qs[dones] = 0.0\n",
    "        target_qs = rewards + (GAMMA * target_qs)\n",
    "\n",
    "        loss = self.policy_net.loss(predicted_qs, target_qs)\n",
    "        self.policy_net.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.policy_net.optimizer.step()\n",
    "\n",
    "        self.total_loss += loss.item()\n",
    "\n",
    "    def save(self):\n",
    "        check_if_dirs_exist([MODELS_PATH])\n",
    "        torch.save(self.policy_net.state_dict(), POLICY_NET_PATH)\n",
    "        torch.save(self.target_net.state_dict(), TARGET_NET_PATH)\n",
    "\n",
    "    def load(self):\n",
    "        self.policy_net.load_state_dict(torch.load(POLICY_NET_PATH))\n",
    "        self.target_net.load_state_dict(torch.load(TARGET_NET_PATH))\n",
    "        self.target_net.eval()\n"
   ],
   "metadata": {
    "id": "ULdu5TPBcW8c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "MAIN"
   ],
   "metadata": {
    "id": "fBYePUh8cXXq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def reset(env: Env):\n",
    "    state, _info = env.reset()\n",
    "    state = preprocess(state)\n",
    "    return state\n",
    "\n",
    "\n",
    "def step(env: Env, action: int):\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    next_state = preprocess(next_state)\n",
    "    done = terminated or truncated\n",
    "    return next_state, reward, done, info\n",
    "\n",
    "\n",
    "def init_memory(env: Env, agent: Agent):\n",
    "    while len(agent.replay_memory) < max(MIN_MEMORY_CAPACITY, BATCH_SIZE):\n",
    "        state = reset(env)\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, info = step(env, action)\n",
    "            agent.replay_memory.store(state, action, reward, done, next_state)\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "def train(env: Env, agent: Agent):\n",
    "    init_memory(env, agent)\n",
    "    print(f\"Memory initialized with {BATCH_SIZE} samples! The training shall begin! Let's rock!\")\n",
    "\n",
    "    reward_history = []\n",
    "    best_score = -np.inf\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        start_time = timeit.default_timer()\n",
    "        state = reset(env)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        step_counter = 0\n",
    "        agent.set_loss(0.0)\n",
    "\n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, info = step(env, action)\n",
    "            agent.replay_memory.store(state, action, reward, done, next_state)\n",
    "            agent.please_learn()\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            step_counter += 1\n",
    "\n",
    "        # We update the target net every episode, one episode has around 4k steps\n",
    "        agent.update_target_net()\n",
    "\n",
    "        agent.decay_epsilon()\n",
    "        reward_history.append(episode_reward)\n",
    "\n",
    "        current_avg_score = np.mean(reward_history[-20:])  # moving average over last 20 episodes\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode + 1} | Reward: {episode_reward} | Avg Reward: {current_avg_score} | Epsilon: {agent.epsilon}\"\n",
    "        )\n",
    "        print(f\"Avg Loss: {agent.total_loss / max(step_counter, 1)} | Steps: {step_counter}\")\n",
    "        print(f\"Episode {episode + 1} took {timeit.default_timer() - start_time} seconds.\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "        if current_avg_score > best_score:\n",
    "            best_score = current_avg_score\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env: Env = make(\"ALE/Skiing-v5\")\n",
    "    agent = Agent(action_space=env.action_space)\n",
    "    train(env, agent)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XjBcInSHcX6d",
    "outputId": "d146fd06-9311-4dd8-f9c3-e5a81484319c"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
